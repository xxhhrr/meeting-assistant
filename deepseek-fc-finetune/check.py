from datasets import load_dataset
from transformers import AutoTokenizer
import json, textwrap

# ============ å®šä¹‰ format_sample ============
def format_sample(example_or_batch):
    is_batched = isinstance(example_or_batch["instruction"], list)

    def _one(instr, steps):
        step_txt = json.dumps(steps, ensure_ascii=False, indent=0)
        return f"### Instruction:\n{instr}\n\n### Output:\n{step_txt}\n"

    if not is_batched:
        return [_one(example_or_batch["instruction"], example_or_batch["output"])]
    else:
        return [_one(i, o) for i, o in zip(example_or_batch["instruction"], example_or_batch["output"])]

# ============ åŠ è½½æ•°æ®é›† ============
ds = load_dataset("json", data_files="data/train.jsonl")["train"]
print("âœ… Dataset loaded:", len(ds), "samples")

# ============ åŠ è½½ tokenizer ============
tok = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-llm-7b-chat", trust_remote_code=True)
tok.pad_token = tok.eos_token

# ============ å…¨é‡æ£€æŸ¥å‡½æ•° ============
def full_sanity_check(dataset):
    bad_control_count = 0
    unk_token_count = 0

    for idx, ex in enumerate(dataset):
        prompt = format_sample(ex)[0]
        bad_chars = [c for c in prompt if ord(c) < 32 and c not in ['\n', '\t']]
        if bad_chars:
            print(f"[{idx}] âš ï¸ æ§åˆ¶å­—ç¬¦: {bad_chars}")
            bad_control_count += 1

        tokens = tok(prompt)["input_ids"]
        unk = tokens.count(tok.unk_token_id)
        if unk > 0:
            print(f"[{idx}] âš ï¸ UNK tokens: {unk} â†’ {textwrap.shorten(prompt, 100)}")
            unk_token_count += 1

    print("\nğŸ“Š æ£€æŸ¥å®Œæˆ")
    print("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”")
    print("æ§åˆ¶å­—ç¬¦å¼‚å¸¸æ ·æœ¬æ•°é‡:", bad_control_count)
    print("åŒ…å« UNK token çš„æ ·æœ¬æ•°é‡:", unk_token_count)
    print("æ€»æ ·æœ¬æ•°é‡:", len(dataset))
    print("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”")

# ============ æ‰§è¡Œæ£€æŸ¥ ============
full_sanity_check(ds)
