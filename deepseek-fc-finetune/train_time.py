# main.py
import json
import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. æ•°æ®ï¼ˆçº¯ JSON â†’ Datasetï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# data_path = "deepseek-fc-finetune/data/train.jsonl"        # â† ä½ çš„åŸå§‹æ–‡ä»¶
data_path = "data/train.jsonl"
raw_ds = load_dataset("json", data_files=data_path)["train"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. Tokenizer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
base_model_id = "deepseek-ai/deepseek-llm-7b-chat"
tok = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)
tok.pad_token = tok.eos_token
tok.padding_side = "right"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. æ ¼å¼åŒ–å‡½æ•° (å¿…å¡«) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def formatting_func(example):
    """
    å°†ä¸€æ¡åŸå§‹æ ·æœ¬æ‹¼æˆçº¯æ–‡æœ¬ promptï¼š
    ### Instruction:
    ...
    
    ### Output:
    ...
    """
    prompt = f"### Instruction:\n{example['instruction']}\n\n### Output:\n"
    output = json.dumps(example["output"], ensure_ascii=False)
    formatted = f"<|user|>\n{prompt}\n<|assistant|>\n{output}"
    return [formatted]  # æ³¨æ„è¦è¿”å› list[str]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. åŸºåº§æ¨¡å‹ + LoRA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
base = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    device_map="auto",
    torch_dtype="auto",          # è‡ªåŠ¨ FP16 / BF16 å–å†³äº GPU
    trust_remote_code=True,
)

lora_cfg = LoraConfig(
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
)
model = get_peft_model(base, lora_cfg)
model.enable_input_require_grads()
model.gradient_checkpointing_enable()
model.config.use_cache = False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. è®­ç»ƒå‚æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
train_args = TrainingArguments(
    output_dir="output",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    learning_rate=5e-5,
    fp16=False, bf16=False,              # å…ˆç”¨çº¯ FP32ï¼›è·‘é€šå†é…Œæƒ…æ”¹ FP16
    logging_steps=5,
    remove_unused_columns=False,         # å¿…é¡»ï¼Œå¦åˆ™ labels ä¼šè¢«åˆ 
    max_grad_norm=0.3,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. SFTTrainer (è€ç‰ˆæœ¬ç”¨æ³•) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
trainer = SFTTrainer(
    model=model,
    args=train_args,
    train_dataset=raw_ds,                # æ³¨æ„ï¼šä¼  **æœª token åŒ–** çš„ Dataset
    tokenizer=tok,
    formatting_func=formatting_func,     # â˜… å…³é”®ï¼šå‘Šè¯‰ SFTTrainer å¦‚ä½•æ‹¼æ–‡æœ¬
    max_seq_length=512,                # åºåˆ—æˆªæ–­é•¿åº¦
    dataset_batch_size=1,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. è®­ç»ƒ & ä¿å­˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
n_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
n_total = sum(p.numel() for p in model.parameters())
print(f"ğŸ” å¯è®­ç»ƒå‚æ•°æ•°é‡: {n_trainable:,} / {n_total:,} ({n_trainable / n_total:.4%})")
trainer.train()
trainer.save_model("output/deepseek-lora")
