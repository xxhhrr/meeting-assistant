from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch
import json

# åŠ è½½æ¨¡å‹
base_model_id = "deepseek-ai/deepseek-llm-7b-chat"
tok = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)
tok.pad_token = tok.eos_token

base = AutoModelForCausalLM.from_pretrained(base_model_id, device_map="auto", torch_dtype="auto", trust_remote_code=True)
model = PeftModel.from_pretrained(base, "deepseek-fc-finetune/output/deepseek-lora")

# æ„é€ æ¨ç† prompt

instruction = "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä¼šè®®åŠ©æ‰‹ï¼Œæ ¹æ®æˆ‘æä¾›çš„ä¿¡æ¯æå–å‡ºtitleï¼Œstart timeï¼Œduration minutesï¼Œattendeesã€‚å½“å‰æ—¶é—´æ˜¯ 2025-06-16ï¼ˆ(Monday)ï¼‰10:40ï¼Œè¯·å®‰æ’ä¸‹å‘¨ä¸‰æ™šä¸Šå…«ç‚¹å’Œé©¬ä¿Šé£å¼€ä¸€ä¸ªäº‘å—æ—…æ¸¸è®¡åˆ’å•†è®®ï¼ŒæŒç»­60åˆ†é’Ÿã€‚"
prompt = f"<|user|>\n### Instruction:\n{instruction}\n\n<|assistant|>\n"

# æ¨ç†
input_ids = tok(prompt, return_tensors="pt").input_ids.to(model.device)
with torch.no_grad():
    outputs = model.generate(input_ids, max_new_tokens=150)

# è§£ç ç»“æœ
response = tok.decode(outputs[0], skip_special_tokens=True)
print("ğŸ§¾ æ¨¡å‹è¾“å‡ºï¼š")
print(response)
# main.py
# import datetime, sys
# from agent.agent_builder import get_meeting_agent
# from prompt_templates import build_prompt

# agent = get_meeting_agent()

# if __name__ == "__main__":
#     if len(sys.argv) > 1:
#         user_req = " ".join(sys.argv[1:])
#     else:
#         user_req = input("â“ è¯·è¾“å…¥ä¼šè®®è¯·æ±‚ï¼š")

#     now_str = datetime.datetime.now()hash -r

#     final_prompt = build_prompt(now_str, user_req)

#     result = agent.invoke(final_prompt)
#     print("ğŸ”¹ Agent è¿”å›ï¼š", result)
# import json, datetime
# from langchain.output_parsers import JsonOutputParser
# from langchain.schema import StrOutputParser
# from langchain.schema.runnable import RunnableLambda, RunnablePassthrough, RunnableSequence
# from llm.local_llm import load_local_chat
# from prompt_templates import SYSTEM_TEMPLATE, build_prompt
# from tools import create_meeting_tool

# # â‘  å›ºå®š system + ç”¨æˆ· prompt
# def build_messages(user_request:str):
#     now_str = datetime.datetime.now().strftime("%Y-%m-%dï¼ˆ%Aï¼‰%H:%M")
#     prompt  = build_prompt(now_str, user_request)
#     return [ {"role":"system","content": SYSTEM_TEMPLATE},
#              {"role":"user",  "content": prompt} ]

# # â‘¡ LLM
# llm = load_local_chat()

# # â‘¢ JsonOutputParserï¼ˆç¡®ä¿ä¸¥æ ¼ JSONï¼‰
# parser = JsonOutputParser()

# # â‘£ Tool executor  (wrap StructuredTool.invoke)
# def call_tool(data: dict):
#     if data.get("name") == "create_meeting":
#         return create_meeting_tool.invoke(data["arguments"])
#     else:
#         return data["arguments"]["reason"]

# # â‘¤ ä¸²æˆå¯æ‰§è¡Œé“¾
# chain: RunnableSequence = (
#     RunnablePassthrough()                   # ä¼ å…¥ user_request
#     | RunnableLambda(lambda req: build_messages(req))   # æ„é€  messages
#     | llm                                    # ç”Ÿæˆ
#     | StrOutputParser()                      # å–çº¯å­—ç¬¦ä¸²
#     | parser                                 # è§£æ JSON
#     | RunnableLambda(call_tool)              # è°ƒç”¨å·¥å…·
# )

# if __name__ == "__main__":
#     user_req = input("â“ è¯·è¾“å…¥ä¼šè®®è¯·æ±‚ï¼š")
#     result = chain.invoke(user_req)
#     print("ğŸ”§ æœ€ç»ˆç»“æœ:", result)
