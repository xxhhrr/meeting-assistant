from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ä½¿ç”¨ DeepSeek å®˜æ–¹æ¨¡å‹
model_id = "deepseek-ai/deepseek-llm-7b-chat"

# åŠ è½½ tokenizer å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype=torch.float16)

# è®¾ç½® tokenizer å‚æ•°
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# æ„é€  promptï¼ˆè¾“å…¥å†…å®¹ï¼‰
prompt = "è¯·å‘Šè¯‰æˆ‘æ˜å¤©æ˜¯å‡ æœˆå‡ å·ï¼Œä½ å¯ä»¥é€šè¿‡"

# ç¼–ç è¾“å…¥
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

# ç”Ÿæˆæ¨ç†ç»“æœ
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=1024,
        do_sample=False,
        temperature=0.1,
    )

# è§£ç è¾“å‡º
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("\nğŸ•’ æ¨¡å‹æ¨ç†ç»“æœï¼š\n")
print(response)
# # import datetime

# # # å®šä¹‰å½“å‰æ—¥æœŸå’Œæ—¶é—´
# # now = datetime.datetime.now()
# # print(now.weekday())
# # # å®šä¹‰è¦è½¬æ¢çš„æ—¥æœŸå’Œæ—¶é—´
# # next_wednesday_11am = now + datetime.timedelta(days=(6-now.weekday()) + 3, hours=11)

# # # å°†æ—¥æœŸå’Œæ—¶é—´è½¬æ¢ä¸ºUTCæ ¼å¼
# # utc_format = next_wednesday_11am.strftime('%Y-%m-%dT%H:%M:%SZ')

# # print(utc_format)
